name: 🧪 Comprehensive CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'
  
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # ==========================================
  # 📊 Setup and Pre-checks
  # ==========================================
  setup:
    name: 🔧 Setup and Validation
    runs-on: ubuntu-latest
    outputs:
      node-cache-key: ${{ steps.cache-keys.outputs.node }}
      python-cache-key: ${{ steps.cache-keys.outputs.python }}
    steps:
      - name: 📦 Checkout Code
        uses: actions/checkout@v4

      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 🐍 Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: 📥 Install Node Dependencies
        run: npm ci

      - name: 📥 Install Python Dependencies
        run: |
          cd python_backend
          pip install -r requirements.txt

      - name: 🔍 Generate Cache Keys
        id: cache-keys
        run: |
          echo "node=$(echo ${{ hashFiles('package-lock.json') }})" >> $GITHUB_OUTPUT
          echo "python=$(echo ${{ hashFiles('python_backend/requirements.txt') }})" >> $GITHUB_OUTPUT

      - name: ✅ Validate Package Files
        run: |
          npm ls --depth=0
          cd python_backend && pip list

  # ==========================================
  # 🔍 Code Quality and Static Analysis
  # ==========================================
  code-quality:
    name: 🔍 Code Quality & Linting
    runs-on: ubuntu-latest
    needs: setup
    steps:
      - name: 📦 Checkout Code
        uses: actions/checkout@v4

      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 📥 Install Dependencies
        run: npm ci

      - name: 🔍 ESLint Check
        run: |
          if [ -f ".eslintrc.js" ] || [ -f ".eslintrc.json" ] || [ -f "eslint.config.js" ]; then
            npm run lint 2>/dev/null || echo "⚠️ ESLint not configured, skipping..."
          else
            echo "⚠️ ESLint not configured, skipping..."
          fi

      - name: 🎨 Prettier Check
        run: |
          if [ -f ".prettierrc" ] || [ -f ".prettierrc.json" ] || [ -f "prettier.config.js" ]; then
            npx prettier --check . || echo "⚠️ Prettier not configured, skipping..."
          else
            echo "⚠️ Prettier not configured, skipping..."
          fi

      - name: 📝 TypeScript Type Check
        run: npm run check

      - name: 🏗️ Build Check
        run: npm run build

  # ==========================================
  # 🧪 Unit Tests
  # ==========================================
  unit-tests:
    name: 🧪 Unit Tests (Jest)
    runs-on: ubuntu-latest
    needs: setup
    steps:
      - name: 📦 Checkout Code
        uses: actions/checkout@v4

      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 📥 Install Dependencies
        run: npm ci

      - name: 🧪 Run Unit Tests
        run: npm run test:unit

      - name: 📊 Generate Coverage Report
        run: npm run test:coverage

      - name: 📤 Upload Coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./test-reports/coverage/lcov.info
          flags: unit-tests
          name: unit-tests-coverage
          fail_ci_if_error: false

      - name: 📤 Upload Coverage Reports
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-coverage
          path: test-reports/coverage/

  # ==========================================
  # 🎭 E2E Tests
  # ==========================================
  e2e-tests:
    name: 🎭 E2E Tests
    runs-on: ubuntu-latest
    needs: setup
    strategy:
      matrix:
        browser: [chromium, firefox, webkit]
      fail-fast: false
    steps:
      - name: 📦 Checkout Code
        uses: actions/checkout@v4

      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 🐍 Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: 📥 Install Dependencies
        run: |
          npm ci
          cd python_backend && pip install -r requirements.txt

      - name: 🎭 Install Playwright Browsers
        run: npx playwright install --with-deps ${{ matrix.browser }}

      - name: 🏗️ Build Application
        run: npm run build

      - name: 🚀 Start Services
        run: |
          # Start Python backend
          cd python_backend && uvicorn main:app --host 0.0.0.0 --port 8000 &
          # Start Express server
          npm start &
          # Wait for services to be ready
          sleep 15

      - name: 🔍 Health Check
        run: |
          curl -f http://localhost:5000/api/health || exit 1
          curl -f http://localhost:8000/health || exit 1

      - name: 🎭 Run E2E Tests
        run: npx playwright test --project=${{ matrix.browser }}

      - name: 📤 Upload Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-results-${{ matrix.browser }}
          path: |
            playwright-report/
            test-results/

  # ==========================================
  # 🐍 Python Backend Tests
  # ==========================================
  python-tests:
    name: 🐍 Python Backend Tests
    runs-on: ubuntu-latest
    needs: setup
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
    steps:
      - name: 📦 Checkout Code
        uses: actions/checkout@v4

      - name: 🐍 Setup Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: 📥 Install Python Dependencies
        run: |
          cd python_backend
          pip install -r requirements.txt
          pip install pytest pytest-cov

      - name: 🧪 Run Python Backend Tests
        run: |
          cd python_backend
          python test_backend.py

      - name: 🧪 Run Python Tests with Pytest (if available)
        run: |
          cd python_backend
          if ls test_*.py 1> /dev/null 2>&1; then
            pytest --cov=. --cov-report=xml --cov-report=term
          else
            echo "No pytest files found, skipping pytest run"
          fi

      - name: 📤 Upload Python Coverage
        uses: codecov/codecov-action@v3
        if: matrix.python-version == '3.11'
        with:
          file: ./python_backend/coverage.xml
          flags: python-tests
          name: python-tests-coverage
          fail_ci_if_error: false

  # ==========================================
  # 🔗 Integration Tests
  # ==========================================
  integration-tests:
    name: 🔗 Integration Tests
    runs-on: ubuntu-latest
    needs: setup
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: debug_player_test
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    steps:
      - name: 📦 Checkout Code
        uses: actions/checkout@v4

      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 🐍 Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: 📥 Install Dependencies
        run: |
          npm ci
          cd python_backend && pip install -r requirements.txt

      - name: 🏗️ Build Application
        run: npm run build

      - name: 🚀 Start Backend Services
        run: |
          # Start Python backend
          cd python_backend && uvicorn main:app --host 0.0.0.0 --port 8000 &
          # Start Express server
          npm start &
          # Wait for services
          sleep 15

      - name: 🔍 Service Health Checks
        run: |
          curl -f http://localhost:5000/api/health || exit 1
          curl -f http://localhost:8000/health || exit 1

      - name: 🧪 Run Frontend Integration Tests
        run: node test-frontend-integration.cjs

      - name: 🧪 Run Integration Flow Tests
        run: python test-integration-flow.py

      - name: 🧪 Run Basic Flow Tests
        run: python test_basic_flows.py

      - name: 📤 Upload Integration Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: test-reports/

  # ==========================================
  # ⚡ Performance Tests
  # ==========================================
  performance-tests:
    name: ⚡ Performance Tests
    runs-on: ubuntu-latest
    needs: setup
    steps:
      - name: 📦 Checkout Code
        uses: actions/checkout@v4

      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 🐍 Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: 📥 Install Dependencies
        run: |
          npm ci
          cd python_backend && pip install -r requirements.txt

      - name: 🎭 Install Playwright
        run: npx playwright install --with-deps chromium

      - name: 🏗️ Build Application
        run: npm run build

      - name: 🚀 Start Services
        run: |
          cd python_backend && uvicorn main:app --host 0.0.0.0 --port 8000 &
          npm start &
          sleep 15

      - name: ⚡ Run Performance Tests
        run: node test-performance.cjs

      - name: 📊 Run Benchmark Tests
        run: |
          if [ -f "scripts/benchmark-performance.js" ]; then
            npm run benchmark
          else
            echo "No benchmark script found, skipping..."
          fi

      - name: 📤 Upload Performance Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results
          path: test-reports/

  # ==========================================
  # 📚 Storybook Tests
  # ==========================================
  storybook-tests:
    name: 📚 Storybook Build & Tests
    runs-on: ubuntu-latest
    needs: setup
    steps:
      - name: 📦 Checkout Code
        uses: actions/checkout@v4

      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 📥 Install Dependencies
        run: npm ci

      - name: 📚 Build Storybook
        run: npm run build-storybook

      - name: 📤 Upload Storybook Build
        uses: actions/upload-artifact@v4
        with:
          name: storybook-build
          path: storybook-static/

  # ==========================================
  # 🔐 Security Tests
  # ==========================================
  security-tests:
    name: 🔐 Security Scan
    runs-on: ubuntu-latest
    needs: setup
    steps:
      - name: 📦 Checkout Code
        uses: actions/checkout@v4

      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: 📥 Install Dependencies
        run: npm ci

      - name: 🔐 Run npm audit
        run: npm audit --audit-level=moderate

      - name: 🔍 Run CodeQL Analysis
        uses: github/codeql-action/init@v2
        with:
          languages: javascript, python

      - name: 🏗️ Autobuild
        uses: github/codeql-action/autobuild@v2

      - name: 🔍 Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v2

  # ==========================================
  # 🐳 Docker Tests
  # ==========================================
  docker-tests:
    name: 🐳 Docker Build & Test
    runs-on: ubuntu-latest
    needs: setup
    steps:
      - name: 📦 Checkout Code
        uses: actions/checkout@v4

      - name: 🐳 Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: 🏗️ Build Docker Image
        run: |
          if [ -f "Dockerfile" ]; then
            docker build -t debug-player-test .
          else
            echo "No Dockerfile found, skipping Docker tests"
            exit 0
          fi

      - name: 🧪 Test Docker Container
        run: |
          if [ -f "Dockerfile" ]; then
            docker run --rm -d --name debug-player-test -p 5000:5000 debug-player-test
            sleep 10
            curl -f http://localhost:5000/api/health || exit 1
            docker stop debug-player-test
          else
            echo "No Dockerfile found, skipping Docker tests"
          fi

  # ==========================================
  # 📊 Comprehensive Test Summary
  # ==========================================
  test-summary:
    name: 📊 Test Results Summary
    runs-on: ubuntu-latest
    needs: [
      code-quality,
      unit-tests,
      e2e-tests,
      python-tests,
      integration-tests,
      performance-tests,
      storybook-tests,
      security-tests,
      docker-tests
    ]
    if: always()
    steps:
      - name: 📊 Generate Test Summary
        run: |
          echo "## 🧪 Comprehensive Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Suite | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Code Quality & Linting | ${{ needs.code-quality.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Unit Tests (Jest) | ${{ needs.unit-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| E2E Tests (Playwright) | ${{ needs.e2e-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Python Backend Tests | ${{ needs.python-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Integration Tests | ${{ needs.integration-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Performance Tests | ${{ needs.performance-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Storybook Build | ${{ needs.storybook-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Security Scan | ${{ needs.security-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Docker Tests | ${{ needs.docker-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### 📈 Overall Status" >> $GITHUB_STEP_SUMMARY
          if [[ "${{ needs.code-quality.result }}" == "success" && 
                "${{ needs.unit-tests.result }}" == "success" && 
                "${{ needs.e2e-tests.result }}" == "success" && 
                "${{ needs.python-tests.result }}" == "success" && 
                "${{ needs.integration-tests.result }}" == "success" ]]; then
            echo "🎉 **All critical tests passed!**" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ **Some tests failed. Please review the results above.**" >> $GITHUB_STEP_SUMMARY
          fi

      - name: ✅ Set Success Status
        if: >
          needs.code-quality.result == 'success' &&
          needs.unit-tests.result == 'success' &&
          needs.e2e-tests.result == 'success' &&
          needs.python-tests.result == 'success' &&
          needs.integration-tests.result == 'success'
        run: echo "🎉 All critical tests passed!"

      - name: ❌ Set Failure Status
        if: >
          needs.code-quality.result != 'success' ||
          needs.unit-tests.result != 'success' ||
          needs.e2e-tests.result != 'success' ||
          needs.python-tests.result != 'success' ||
          needs.integration-tests.result != 'success'
        run: |
          echo "❌ Critical tests failed!"
          exit 1