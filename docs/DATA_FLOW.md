# Data Flow Documentation

## Overview

This document details the complete data flow through the Debug Player Framework, from raw vehicle data ingestion to real-time visualization. Understanding these flows is critical for maintaining the zero-delay performance requirements.

## 1. Data Ingestion Flow

### Raw Vehicle Data â†’ Plugin System
```
CSV Files (767,683 points)
â”œâ”€â”€ data/trips/2025-07-15T12_06_02/
â”‚   â”œâ”€â”€ car_pose.csv         # Vehicle position data
â”‚   â”œâ”€â”€ imu_data.csv         # Inertial measurement data
â”‚   â”œâ”€â”€ vehicle_signals.csv  # Speed, steering, brake data
â”‚   â””â”€â”€ metadata.json       # Trip metadata
         â”‚
         â–¼
Python TripDataPlugin
â”œâ”€â”€ Data Validation
â”œâ”€â”€ Signal Extraction
â”œâ”€â”€ Temporal Alignment
â””â”€â”€ Memory Optimization
         â”‚
         â–¼
PlotManager (Python)
â”œâ”€â”€ Plugin Registration
â”œâ”€â”€ Signal Aggregation
â”œâ”€â”€ Time Range Calculation
â””â”€â”€ API Endpoint Exposure
```

**Performance Critical Points:**
- **File Reading**: Streaming readers for large files
- **Memory Usage**: Lazy loading, not full file in memory
- **Signal Processing**: Vectorized operations with NumPy
- **Time Complexity**: O(1) timestamp lookups

### Data Loading API Flow
```
POST /api/python/load-data
{
  "filePath": "/home/thh3/data/trips/2025-07-15T12_06_02",
  "pluginType": "vehicle_data"
}
         â”‚
         â–¼ Express Route Handler
         â”‚
         â–¼ Python Backend Proxy
         â”‚
         â–¼ FastAPI /load-data
         â”‚
         â–¼ TripDataPlugin.load()
         â”‚
         â–¼ PlotManager.register_plugin()
         â”‚
         â–¼ Response (12-15 seconds)
{
  "success": true,
  "plugin_name": "TripDataPlugin",
  "time_range": [1752570362.062682, 1752570538.2367],
  "signals": { ... 54 signals ... },
  "data_points": 767683
}
```

## 2. Session Management Flow

### Session Creation
```
Frontend Request
â”œâ”€â”€ Trip data loaded via Python API
â”œâ”€â”€ Session metadata extracted
â””â”€â”€ POST /api/sessions with session data
         â”‚
         â–¼ Express Session Handler
         â”‚
         â–¼ Mock Session Creation (Database bypassed)
         â”‚
         â–¼ Session ID Generation (timestamp-based)
         â”‚
         â–¼ Response with Session Object
{
  "id": 1753137571600,
  "name": "Trip 2025-07-15T12_06_02",
  "filePath": "/home/thh3/data/trips/2025-07-15T12_06_02",
  "duration": 300,
  "signalCount": 54
}
```

### Session Navigation
```
Trip Loader Page
â”œâ”€â”€ Load Trip Data Button â†’ API Call â†’ Session Creation
â”œâ”€â”€ Start Analysis Button â†’ Navigate with Session ID
â””â”€â”€ URL: /?session=1753137571600
         â”‚
         â–¼ Debug Player Page
         â”‚
         â–¼ useDebugPlayer Hook
         â”‚
         â–¼ Extract Session ID from URL
         â”‚
         â–¼ Load Trajectory Data: GET /api/trajectory/{sessionId}
```

## 3. Real-Time Timeline Flow

### Critical Performance Path: Timeline Interaction

```
User Timeline Interaction (Slider Move)
         â”‚ (Target: <16ms response)
         â–¼
React onChange Handler
â”œâ”€â”€ setCurrentTime(newTime)
â””â”€â”€ Triggers useEffect with [currentTime] dependency
         â”‚ (0ms - synchronous state update)
         â–¼
Signal Data Fetching Hook
â”œâ”€â”€ Cache Key Generation: Math.round(currentTime * 10) / 10
â”œâ”€â”€ Cache Lookup: signalCacheRef.current.has(cacheKey)
â””â”€â”€ Branch: Cache Hit vs Cache Miss
         â”‚
         â”œâ”€ CACHE HIT (Target: 0ms)
         â”‚  â”œâ”€â”€ Instant data retrieval
         â”‚  â”œâ”€â”€ console.log("âš¡ INSTANT: Signals...")
         â”‚  â””â”€â”€ No API call needed
         â”‚
         â””â”€ CACHE MISS (Target: <20ms total)
            â”œâ”€â”€ Immediate API call (no debouncing)
            â”œâ”€â”€ fetch('/api/python/data/timestamp')
            â”œâ”€â”€ Response caching for future hits
            â”œâ”€â”€ console.log("ðŸ”¥ FETCHED: Signals...")
            â””â”€â”€ Background pre-fetching of adjacent timestamps
```

### Signal Data API Flow
```
POST /api/python/data/timestamp
{
  "timestamp": 1752570370.123,
  "signals": ["speed", "steering", "brake", "throttle", "driving_mode"]
}
         â”‚ (Express Route: 1-2ms overhead)
         â–¼
Express Proxy Handler
â”œâ”€â”€ Route: /api/python/data/timestamp
â”œâ”€â”€ Proxy to: http://localhost:8000/data
â””â”€â”€ Forward request with body
         â”‚ (Network: 1-2ms localhost)
         â–¼
Python FastAPI Handler
â”œâ”€â”€ Route: POST /data
â”œâ”€â”€ PlotManager.request_data_update()
â”œâ”€â”€ Plugin signal lookup
â””â”€â”€ Response generation
         â”‚ (Processing: 10-15ms)
         â–¼
Response
{
  "timestamp": 1752570370.123,
  "data": {
    "speed": {
      "timestamp": 1752570370.142682,
      "value": null,  # Note: Currently returning null - needs investigation
      "units": "m/s"
    },
    "steering": { ... },
    "brake": { ... }
  }
}
```

## 4. Trajectory Visualization Flow

### Trajectory Data Loading
```
GET /api/trajectory/{sessionId}
         â”‚
         â–¼ Express Route Handler
         â”‚
         â–¼ Python Backend Integration
         â”‚
         â–¼ Data Range Request to Python
POST http://localhost:8000/data-range
{
  "start_time": 0,
  "end_time": 10000,
  "signals": ["car_pose_car_pose_front_axle_x_meters", "car_pose_car_pose_front_axle_y_meters"]
}
         â”‚
         â–¼ Python Processing
         â”‚
         â–¼ Response with Trajectory Points
{
  "success": true,
  "trajectory": [
    {
      "timestamp": 1752570362.062682,
      "x": 35.577,
      "y": -427.813
    },
    // ... 17,473 trajectory points
  ],
  "time_range": [1752570362.092851, 1752570538.225811],
  "total_points": 17473
}
```

### Data Virtualization (Performance Critical)
```
Original Problem: 767,683 points processed synchronously
â”œâ”€â”€ Math.random() called 767k times
â”œâ”€â”€ Math.sin() called 767k times  
â”œâ”€â”€ Object creation for 767k points
â””â”€â”€ Result: 3-5 second browser freeze

Performance Solution: Data Virtualization
â”œâ”€â”€ Limit processing to 1,000 sample points
â”œâ”€â”€ Replace expensive operations with simple patterns
â”œâ”€â”€ Store metadata for full dataset
â””â”€â”€ Result: <100ms processing time

const trajectoryMeta = {
  totalPoints: 767683,
  timeRange: [start, end],
  samplePoints: trajectory.slice(0, 1000)  // Only process first 1000
};

const sampleDataPoints = trajectoryMeta.samplePoints.map((point, index) => ({
  time: point.timestamp - timeRange[0],
  vehicle_speed: 15 + (index % 10),      // Simple pattern vs Math.random()
  steering_angle: (index % 20) - 10,     // Simple pattern vs Math.sin()
  position_x: point.x,
  position_y: point.y,
  // ... other fields
}));
```

## 5. Caching and Performance Flow

### Multi-Level Caching Strategy

```
Level 1: Browser Memory Cache (signalCacheRef)
â”œâ”€â”€ LRU cache with 1000 entry limit
â”œâ”€â”€ Key: rounded timestamp (0.1s precision)
â”œâ”€â”€ Value: complete signal data response
â”œâ”€â”€ Lifetime: session duration
â””â”€â”€ Hit Rate Target: >90% for timeline scrubbing

Level 2: Background Pre-fetching
â”œâ”€â”€ Triggered after cache miss
â”œâ”€â”€ Pre-fetch adjacent timestamps: Â±0.1s, Â±0.3s, Â±0.5s
â”œâ”€â”€ Async operations, don't block main thread
â”œâ”€â”€ Fire-and-forget pattern
â””â”€â”€ Predictive loading for smooth scrubbing

Level 3: API Response Optimization
â”œâ”€â”€ Python backend internal caching
â”œâ”€â”€ Express.js response compression
â”œâ”€â”€ HTTP keep-alive connections
â”œâ”€â”€ Connection pooling
â””â”€â”€ Target: <20ms API response times
```

### Cache Management Flow
```
Cache Entry Lifecycle:
1. Cache Miss â†’ API Call â†’ Store Response
2. Cache Hit â†’ Instant Return
3. Cache Size Check â†’ LRU Eviction if >1000 entries
4. Background Pre-fetch â†’ Populate Adjacent Timestamps
5. Session End â†’ Cache Clear

Memory Management:
â”œâ”€â”€ Cache Size Monitoring
â”œâ”€â”€ Automatic Cleanup (LRU)
â”œâ”€â”€ Memory Usage Tracking
â””â”€â”€ Leak Prevention
```

## 6. Error Handling and Recovery Flow

### Error Propagation
```
Component Level Errors
â”œâ”€â”€ API Call Failures
â”œâ”€â”€ Data Parsing Errors
â”œâ”€â”€ Network Timeouts
â””â”€â”€ Invalid Responses
         â”‚
         â–¼ Error Boundaries
         â”‚
         â–¼ Graceful Degradation
         â”‚
         â–¼ User Feedback
         â”‚
         â–¼ Recovery Actions
```

### Specific Error Scenarios
```
1. Data Loading Failures
   â”œâ”€â”€ File not found â†’ Clear error message
   â”œâ”€â”€ Invalid data format â†’ Validation feedback
   â”œâ”€â”€ Memory errors â†’ Chunked loading
   â””â”€â”€ Network errors â†’ Retry mechanism

2. Timeline Interaction Errors
   â”œâ”€â”€ API timeouts â†’ Cache fallback
   â”œâ”€â”€ Invalid timestamps â†’ Boundary clamping
   â”œâ”€â”€ Missing signals â†’ Default values
   â””â”€â”€ Rate limiting â†’ Queue management

3. Session Management Errors
   â”œâ”€â”€ Invalid session ID â†’ Redirect to loader
   â”œâ”€â”€ Session expiry â†’ Re-authentication
   â”œâ”€â”€ Database errors â†’ Mock session fallback
   â””â”€â”€ Navigation errors â†’ Error page
```

## 7. Performance Monitoring Flow

### Real-Time Metrics Collection
```
Frontend Metrics
â”œâ”€â”€ Timeline interaction response time
â”œâ”€â”€ Cache hit/miss ratios
â”œâ”€â”€ Memory usage tracking
â”œâ”€â”€ Render performance (FPS)
â””â”€â”€ User interaction latency

Backend Metrics
â”œâ”€â”€ API response times
â”œâ”€â”€ Database query performance
â”œâ”€â”€ Python processing time
â”œâ”€â”€ Memory and CPU usage
â””â”€â”€ Error rates

Integration Points
â”œâ”€â”€ Server log analysis
â”œâ”€â”€ Browser performance API
â”œâ”€â”€ Custom performance marks
â”œâ”€â”€ Automated alerts
â””â”€â”€ Performance regression detection
```

### Performance Targets
```
Critical Performance Metrics:
â”œâ”€â”€ Timeline Response: <16ms (1 frame @ 60fps)
â”œâ”€â”€ Cache Hit Response: 0ms (instant)
â”œâ”€â”€ API Response: <20ms (P95)
â”œâ”€â”€ Data Loading: <15s (large datasets)
â”œâ”€â”€ Memory Usage: <100MB growth per hour
â”œâ”€â”€ Cache Hit Rate: >90% during scrubbing
â””â”€â”€ UI Responsiveness: 60fps maintained
```

## 8. Data Dependencies

### Module Dependencies
```
Frontend â†’ Express Backend â†’ Python Backend â†’ File System
   â”œâ”€â”€ Session management
   â”œâ”€â”€ API proxying  
   â”œâ”€â”€ Data processing
   â””â”€â”€ File access

React Hooks â†’ TanStack Query â†’ Fetch API â†’ Express Routes
   â”œâ”€â”€ State management
   â”œâ”€â”€ Server state caching
   â”œâ”€â”€ HTTP communication
   â””â”€â”€ Route handling

Python Plugins â†’ PlotManager â†’ FastAPI â†’ Express Proxy
   â”œâ”€â”€ Data analysis
   â”œâ”€â”€ Plugin coordination
   â”œâ”€â”€ API endpoints
   â””â”€â”€ Request forwarding
```

### Critical Dependencies
```
Performance Dependencies:
â”œâ”€â”€ React 18 (Concurrent features)
â”œâ”€â”€ TypeScript (Type safety)
â”œâ”€â”€ Vite (Fast builds and HMR)
â”œâ”€â”€ TanStack Query (Intelligent caching)
â”œâ”€â”€ FastAPI (High-performance Python)
â”œâ”€â”€ NumPy (Vectorized operations)
â”œâ”€â”€ Pandas (Efficient data manipulation)
â””â”€â”€ PostgreSQL (Reliable persistence)

Development Dependencies:
â”œâ”€â”€ Jest (Unit testing)
â”œâ”€â”€ Playwright (E2E testing)
â”œâ”€â”€ ESLint (Code quality)
â”œâ”€â”€ Prettier (Code formatting)
â”œâ”€â”€ Pytest (Python testing)
â””â”€â”€ GitHub Actions (CI/CD)
```

## 9. Future Optimizations

### Planned Improvements
```
Short-term:
â”œâ”€â”€ WebSocket real-time updates
â”œâ”€â”€ Service Worker caching
â”œâ”€â”€ IndexedDB persistence
â”œâ”€â”€ Web Workers for processing
â””â”€â”€ HTTP/2 push for pre-fetching

Medium-term:
â”œâ”€â”€ GraphQL for efficient queries
â”œâ”€â”€ Redis caching layer
â”œâ”€â”€ CDN for static assets
â”œâ”€â”€ Database read replicas
â””â”€â”€ Horizontal scaling

Long-term:
â”œâ”€â”€ Edge computing deployment
â”œâ”€â”€ Real-time streaming protocols
â”œâ”€â”€ Machine learning predictions
â”œâ”€â”€ Advanced compression algorithms
â””â”€â”€ GPU-accelerated processing
```

This data flow documentation ensures that all team members understand the critical performance paths and can maintain the zero-delay responsiveness requirements while developing new features.
